{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "Archive:  data/ptb.zip\n",
      "  inflating: data/ptb/reader.py      \n",
      "  inflating: data/__MACOSX/ptb/._reader.py  \n",
      "  inflating: data/__MACOSX/._ptb     \n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -q -O data/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "!unzip -o data/ptb.zip -d data\n",
    "!cp data/ptb/reader.py .\n",
    "\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-12 11:03:29--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
      "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: ‘simple-examples.tgz.3’\n",
      "\n",
      "100%[======================================>] 34,869,662  3.36MB/s   in 11s    \n",
      "\n",
      "2020-01-12 11:03:41 (3.14 MB/s) - ‘simple-examples.tgz.3’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 60\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size = 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "x = first_touple[0]\n",
    "y = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361,    8, 1279,  437,  597,    6,  261, 4276, 1089,\n",
       "           8, 2836,    2,  269,    4, 5526,  241,   13, 2420],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {_input_data:x, _targets:y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, ..., 9993, 9994, 9995],\n",
       "       [ 901,   33, 3361, ...,  241,   13, 2420],\n",
       "       [2654,    6,  334, ...,  514,    8,  605],\n",
       "       ...,\n",
       "       [7831,   36, 1678, ...,    4, 4558,  157],\n",
       "       [  59, 2070, 2433, ...,  400,    1, 1173],\n",
       "       [2097,    3,    2, ..., 2043,   23,    1]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-15-ddf565d0e753>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-15-ddf565d0e753>:3: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l1, forget_bias=0.0)\n",
    "lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(hidden_size_l2, forget_bias=0.0)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(60, 256) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(60, 256) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(60, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(60, 128) dtype=float32>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), h=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embedding_vocab = tf.get_variable(\"embedding_vocab\", [vocab_size, embeding_vector_size])  #[10000x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01460936,  0.00786084, -0.01149052, ...,  0.01345345,\n",
       "         0.0028404 , -0.01638505],\n",
       "       [-0.00528103,  0.01078538,  0.01873348, ..., -0.02169878,\n",
       "         0.00603994, -0.02317163],\n",
       "       [-0.01200122, -0.02398775, -0.0073299 , ..., -0.00892472,\n",
       "        -0.0182585 ,  0.00200305],\n",
       "       ...,\n",
       "       [-0.02402098,  0.00651272, -0.00965233, ..., -0.0241305 ,\n",
       "         0.01614254,  0.00356876],\n",
       "       [ 0.01971399,  0.0085628 , -0.00044482, ..., -0.02228205,\n",
       "         0.01607617,  0.00073694],\n",
       "       [-0.02303687, -0.00752103,  0.01365978, ..., -0.00553451,\n",
       "         0.01619715,  0.01931012]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(60, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding_vocab, _input_data)  #shape=(30, 20, 200) \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00673603, -0.00744445,  0.01614888, ..., -0.00271672,\n",
       "         0.00303701,  0.0054742 ],\n",
       "       [-0.00295363,  0.01609612, -0.00360958, ..., -0.0025131 ,\n",
       "        -0.00815064, -0.00774505],\n",
       "       [ 0.02050654,  0.00390306, -0.01488357, ..., -0.02207757,\n",
       "        -0.00656124,  0.0160511 ],\n",
       "       ...,\n",
       "       [-0.01310164,  0.00090433, -0.01015644, ...,  0.01478478,\n",
       "         0.0226698 , -0.00538628],\n",
       "       [-0.00664147,  0.01318635, -0.00522501, ...,  0.00872479,\n",
       "        -0.00966389, -0.00412718],\n",
       "       [-0.00956363, -0.01261902, -0.021668  , ...,  0.02391858,\n",
       "         0.00795539,  0.02075953]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-b4cc29283025>:1: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose_1:0' shape=(60, 20, 128) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5003602e-04,  3.1586166e-04,  2.9241623e-04, ...,\n",
       "         1.1497134e-05,  5.3108251e-05,  2.4800927e-05],\n",
       "       [ 3.1611207e-04, -3.4809143e-06, -2.4679856e-04, ...,\n",
       "         2.5140689e-04,  5.4880843e-04, -2.3084985e-04],\n",
       "       [ 2.9136630e-05, -5.1979754e-05, -3.8078468e-04, ...,\n",
       "         4.5696009e-04,  4.0135210e-04, -5.1611849e-05],\n",
       "       ...,\n",
       "       [-2.8330303e-04, -4.3021605e-04,  1.0475953e-03, ...,\n",
       "         3.7297371e-04, -2.7931307e-04, -5.3723116e-04],\n",
       "       [-2.9873830e-04,  2.2869327e-04,  3.9279470e-04, ...,\n",
       "         2.3815266e-04, -1.5306709e-04, -8.5854769e-04],\n",
       "       [ 4.5843702e-04,  8.1015169e-04,  1.1451463e-05, ...,\n",
       "         9.7776041e-04, -3.4691588e-04, -7.5815007e-04]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(1200, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, hidden_size_l2])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size_l2, vocab_size]) #[200x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output:  (1200, 10000)\n",
      "The probability of observing words in t=0 to t=20 [[1.00697107e-04 1.00585676e-04 1.01352714e-04 ... 9.95345545e-05\n",
      "  9.97396928e-05 1.01624893e-04]\n",
      " [1.00702309e-04 1.00584723e-04 1.01352278e-04 ... 9.95342343e-05\n",
      "  9.97462776e-05 1.01630481e-04]\n",
      " [1.00697340e-04 1.00592486e-04 1.01350743e-04 ... 9.95323353e-05\n",
      "  9.97343086e-05 1.01635131e-04]\n",
      " ...\n",
      " [1.00688107e-04 1.00605343e-04 1.01352307e-04 ... 9.95120863e-05\n",
      "  9.97181705e-05 1.01629907e-04]\n",
      " [1.00697027e-04 1.00604891e-04 1.01348502e-04 ... 9.95114242e-05\n",
      "  9.97245079e-05 1.01627586e-04]\n",
      " [1.00690158e-04 1.00599696e-04 1.01342630e-04 ... 9.95180308e-05\n",
      "  9.97208335e-05 1.01632329e-04]]\n"
     ]
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "output_words_prob = session.run(prob, feed_dict)\n",
    "print(\"shape of the output: \", output_words_prob.shape)\n",
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3254, 9446, 8278, 2643,  790,  790,  790, 6262, 6262, 2643, 2643,\n",
       "       2976, 5746, 6799, 6799, 2643, 2643, 2682, 2682, 2682])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0:20], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = session.run(_targets, feed_dict) \n",
    "targ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.21167 , 9.21361 , 9.212688, 9.204365, 9.219266, 9.202276,\n",
       "       9.198005, 9.204278, 9.223249, 9.227576], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.17181"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_vocab:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(456, 1024) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(128, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = tf.placeholder(tf.float32)\n",
    "var_y = tf.placeholder(tf.float32) \n",
    "func_test = 2.0 * var_x * var_x + 3.0 * var_x * var_y\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(func_test, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_x])\n",
    "session.run(var_grad, {var_x:1.0,var_y:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_y])\n",
    "session.run(var_grad, {var_x:1.0, var_y:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.IndexedSlices at 0x7f6d1854aba8>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul/Enter_grad/b_acc_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/add_grad/Reshape_1:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "#sess.run(grad_t_list,feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.ops.IndexedSlices at 0x7f6d18571438>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(456, 1024) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(384, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(128, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndexedSlicesValue(values=array([[ 1.7227909e-05, -6.0833472e-06, -4.2193201e-06, ...,\n",
       "         -8.8399993e-06, -6.3805082e-06,  8.0483078e-06],\n",
       "        [ 1.7380007e-05, -3.6047475e-06, -4.4444751e-06, ...,\n",
       "         -5.1073516e-06, -8.1322887e-06,  3.0474253e-06],\n",
       "        [ 1.5509744e-05,  1.2050032e-06, -4.1060357e-06, ...,\n",
       "         -7.1236846e-06, -1.2960888e-05, -5.3294571e-06],\n",
       "        ...,\n",
       "        [-3.7329401e-06, -1.2221494e-05,  1.6027481e-06, ...,\n",
       "         -9.8279020e-07, -4.5977135e-06, -9.4978753e-07],\n",
       "        [-1.5101216e-06, -1.2057955e-05,  8.4613130e-06, ...,\n",
       "          1.2869953e-06,  3.6337030e-06, -1.2242238e-06],\n",
       "        [-2.7450180e-06, -5.2215655e-06,  9.9559811e-06, ...,\n",
       "         -3.1048949e-06,  2.7373369e-06, -5.3823555e-06]], dtype=float32), indices=array([9970, 9971, 9972, ..., 2043,   23,    1], dtype=int32), dense_shape=array([10000,   200], dtype=int32)),\n",
       " array([[ 9.72881686e-09, -4.81436047e-09,  5.56496804e-09, ...,\n",
       "          3.47587950e-08, -3.83431553e-09, -2.01389909e-08],\n",
       "        [ 3.12707815e-09, -3.83256697e-08,  2.14325322e-08, ...,\n",
       "          1.06232623e-08,  1.37662690e-08, -2.83833290e-09],\n",
       "        [ 1.32922526e-08, -6.87985136e-08,  1.32169689e-08, ...,\n",
       "          3.34537766e-08,  6.39753583e-10,  2.79843917e-08],\n",
       "        ...,\n",
       "        [-6.85089874e-10,  2.90087310e-09, -1.32774125e-09, ...,\n",
       "         -1.16303065e-08,  7.15099224e-09, -5.21121191e-09],\n",
       "        [ 1.15848287e-09,  4.38308501e-09,  6.11321382e-10, ...,\n",
       "          2.96557756e-09, -1.49921693e-08,  3.66017661e-10],\n",
       "        [ 5.63020919e-09, -1.00545261e-09,  3.91083521e-09, ...,\n",
       "          2.63091682e-09,  4.87080998e-09,  9.05345132e-09]], dtype=float32),\n",
       " array([ 5.2541480e-08,  1.6237165e-06,  1.1761894e-06, ...,\n",
       "        -1.2511756e-06, -3.5728062e-06, -3.9625169e-07], dtype=float32),\n",
       " array([[ 2.4366598e-10, -2.3295756e-08,  5.0029327e-09, ...,\n",
       "          6.6822832e-09,  7.8476681e-10,  3.6304633e-09],\n",
       "        [-5.6285243e-10, -6.1360472e-09,  1.2910103e-08, ...,\n",
       "         -1.0035032e-09, -6.1087140e-09,  1.2388653e-09],\n",
       "        [-8.0770608e-09, -1.1702229e-09, -6.8724542e-09, ...,\n",
       "         -1.7174934e-09, -4.9819876e-10,  3.2573717e-09],\n",
       "        ...,\n",
       "        [-1.5560506e-09,  1.0712494e-09,  1.2839231e-09, ...,\n",
       "         -5.2128186e-09, -3.1492781e-11,  1.1736906e-09],\n",
       "        [-9.1853114e-10, -1.3004606e-09,  6.9326606e-10, ...,\n",
       "          1.0597085e-09,  4.2617052e-09,  1.3081409e-09],\n",
       "        [ 6.3543243e-10, -2.3654105e-09, -5.9620908e-10, ...,\n",
       "          1.7396429e-09,  5.5831045e-10,  3.5223571e-09]], dtype=float32),\n",
       " array([ 4.01912587e-07, -3.75524451e-06,  7.04287686e-06, -7.64361357e-06,\n",
       "         2.29392026e-06, -2.28236013e-06,  7.34990408e-07,  2.41157841e-06,\n",
       "         4.11298015e-06, -8.13498673e-07,  5.89702404e-06,  2.07975540e-06,\n",
       "         3.33410958e-06,  1.12867178e-06,  3.02891999e-06,  2.84945577e-06,\n",
       "        -5.46922865e-06, -6.50145068e-07,  3.53603787e-06, -5.10812868e-07,\n",
       "         4.28641215e-07, -5.04196942e-06, -3.61758066e-06, -9.85550855e-07,\n",
       "         4.56438045e-07, -1.52798364e-07, -2.46879245e-06, -2.67875089e-06,\n",
       "         1.28385034e-06,  2.12079090e-06, -3.86560896e-06,  1.13562669e-06,\n",
       "        -7.51451410e-07,  4.25362896e-06,  6.07499305e-06,  2.37326435e-06,\n",
       "         3.64225139e-06,  6.25245832e-07, -3.88189210e-06,  6.19199136e-06,\n",
       "        -8.30530951e-07,  1.13104761e-06, -5.29802492e-06,  1.10496751e-06,\n",
       "        -1.96748169e-06,  1.46764853e-06, -7.95986216e-06, -3.57234427e-07,\n",
       "        -1.49064431e-06,  9.17685975e-06, -2.19031881e-06,  1.28159783e-07,\n",
       "         2.13709313e-06,  1.14692102e-06,  7.93165555e-07, -2.21692608e-06,\n",
       "        -1.17860566e-06, -4.41714110e-06,  5.73818625e-07,  1.84150849e-07,\n",
       "         6.89203489e-07,  9.99100394e-07, -1.64404207e-06,  4.50765492e-06,\n",
       "         5.92477636e-06, -4.13627595e-06,  4.46714876e-06,  4.12977533e-06,\n",
       "         1.20738810e-06,  1.86917407e-06,  5.28411783e-06, -6.47936395e-06,\n",
       "         3.61179445e-06,  2.07964194e-06, -4.33468540e-06, -2.36541905e-06,\n",
       "         1.06120092e-06,  1.38596221e-07,  3.25191581e-06,  4.97750443e-06,\n",
       "         2.51189203e-06, -5.42970611e-07,  7.29723615e-06,  7.20594608e-06,\n",
       "        -6.63829815e-06,  2.44531361e-06,  5.10076234e-06, -1.87115972e-06,\n",
       "         3.51826884e-07,  5.10027860e-07, -4.66822883e-08, -2.32429397e-06,\n",
       "         3.51253618e-07, -3.58684247e-07, -1.07030121e-06,  5.31984540e-07,\n",
       "        -2.02098846e-07, -9.69964503e-07, -1.69227667e-06, -2.60966044e-06,\n",
       "        -5.02477178e-06, -4.24359268e-06, -2.18420291e-06, -4.83710392e-06,\n",
       "         2.89135232e-06,  7.58767897e-07,  2.07528296e-06, -7.54224993e-06,\n",
       "        -1.92149901e-06, -9.73021997e-07,  5.91109119e-06,  4.16205438e-07,\n",
       "        -1.40684824e-06, -7.41576423e-06,  2.88407932e-06,  6.41358611e-09,\n",
       "        -4.64865298e-06,  2.14206443e-06, -6.62717730e-06, -4.15477416e-06,\n",
       "        -1.83318048e-06, -1.06447351e-06,  9.73860097e-07,  6.27780128e-06,\n",
       "        -1.32026344e-06, -5.79855077e-06, -2.62280741e-06,  7.48331331e-07,\n",
       "        -2.72528990e-03,  1.87117402e-02, -3.25381756e-02,  1.27594722e-02,\n",
       "        -1.45047009e-02, -2.41826139e-02, -2.18108837e-02,  1.12872086e-02,\n",
       "        -3.03052869e-02, -2.18001846e-03,  5.97596588e-03,  3.29939425e-02,\n",
       "         2.32444634e-03, -9.66677908e-03, -4.30172123e-03,  2.31935340e-03,\n",
       "        -3.02821808e-02, -4.93087526e-03, -1.17605776e-02,  2.22182740e-02,\n",
       "         7.69817596e-03,  9.43750888e-03,  2.30957940e-03,  1.08671766e-02,\n",
       "        -1.35073690e-02, -1.98038463e-02,  1.44351088e-02,  1.27179623e-02,\n",
       "         9.74288490e-03,  3.85900540e-03,  1.97036788e-02, -5.62059879e-03,\n",
       "        -5.39587298e-03, -1.65148024e-02, -4.29438390e-02,  4.83024726e-03,\n",
       "        -1.00612873e-02,  1.21810997e-03, -2.50058668e-03, -1.55675346e-02,\n",
       "        -8.01536534e-03, -3.06627713e-03,  2.13241559e-02,  1.18915085e-02,\n",
       "         2.20454112e-02, -3.32042156e-03, -2.53732279e-02, -9.54940263e-03,\n",
       "        -1.75497159e-02,  2.69073136e-02, -5.88821364e-04, -8.20344966e-03,\n",
       "         3.47050279e-03,  1.20651652e-03,  5.79979969e-03,  1.11767696e-02,\n",
       "        -1.53899500e-02,  6.20901934e-04,  1.18062841e-02, -1.22631220e-02,\n",
       "        -2.28354894e-03,  4.72447462e-03,  6.76541356e-03, -6.25452446e-03,\n",
       "         3.38899009e-02, -2.09173141e-03, -8.72268341e-03,  2.76585785e-03,\n",
       "         6.28900155e-03, -2.63275690e-02,  1.74871236e-02,  3.47818322e-02,\n",
       "        -2.03058943e-02,  8.25847557e-04, -6.81721373e-04,  1.27076160e-03,\n",
       "        -1.28374398e-02, -1.01757981e-03, -1.11945774e-02, -1.99978370e-02,\n",
       "        -3.41315530e-02, -1.44603280e-02,  2.51589604e-02,  8.76962021e-03,\n",
       "         2.72190794e-02, -4.32320777e-03, -1.99741200e-02, -1.56596620e-02,\n",
       "         1.28408475e-02,  8.63709580e-03, -9.52131301e-03, -6.46917848e-03,\n",
       "        -3.23264673e-03, -1.55602917e-02,  4.72324854e-03,  8.53893254e-03,\n",
       "         3.10475845e-03, -1.29879303e-02, -6.60126051e-03,  2.47494467e-02,\n",
       "        -2.51373230e-03,  2.62635085e-03, -1.62649564e-02,  1.54570648e-02,\n",
       "         1.65673774e-02,  2.04930007e-02,  1.14595210e-02, -2.15808544e-02,\n",
       "        -6.57726778e-03,  1.58863105e-02, -3.30007821e-02, -1.32685900e-02,\n",
       "        -6.15630019e-03,  3.59421447e-02,  1.28639834e-02,  4.73880302e-03,\n",
       "        -1.75322946e-02, -4.79530683e-03,  1.85286067e-02, -1.17270248e-02,\n",
       "         2.22823420e-03,  1.37111321e-02,  6.21253205e-03, -1.37053793e-02,\n",
       "         4.96912235e-03, -2.43687443e-02,  1.39995236e-02,  1.28245279e-02,\n",
       "        -5.68938958e-07, -2.91423748e-06,  2.86298609e-06, -3.49800575e-06,\n",
       "         1.49455718e-06, -2.14650072e-06,  1.30487052e-07,  5.64252105e-07,\n",
       "         8.99552504e-07, -1.87619685e-06,  3.57622753e-06,  1.52611995e-06,\n",
       "         1.74791012e-06, -1.01132571e-06,  2.16268336e-06,  1.94549926e-07,\n",
       "        -6.22749167e-06, -9.91447223e-07,  2.60722004e-06,  3.88522892e-07,\n",
       "         4.65603904e-07, -1.79048277e-06, -1.32246691e-06, -1.26017682e-07,\n",
       "        -1.83154697e-07,  1.00842567e-06, -1.10845463e-06,  3.44327987e-07,\n",
       "        -2.04600099e-07,  5.50542154e-07, -4.40281019e-06, -2.57821875e-06,\n",
       "        -9.49206310e-07,  1.89579976e-06,  6.80539006e-06, -3.66681036e-07,\n",
       "         3.32158561e-06,  9.07563958e-07, -3.32454442e-06,  3.98787961e-06,\n",
       "        -8.58965677e-07, -1.08020686e-06, -3.05136768e-06,  4.43240793e-07,\n",
       "        -1.96227393e-06,  5.73787361e-07, -5.40652718e-06, -6.48937544e-07,\n",
       "         6.05783271e-07,  8.81579035e-06, -5.65990064e-08, -1.43465593e-06,\n",
       "         1.80832683e-06,  1.35205482e-06,  4.14304054e-07,  9.28525594e-07,\n",
       "        -2.18542323e-06, -9.13022177e-07,  6.54550149e-07,  1.96701365e-07,\n",
       "         5.35466654e-07,  8.05817820e-07, -1.70372448e-06,  1.44982710e-06,\n",
       "         4.09980930e-06, -3.17807417e-06,  2.15089176e-06,  3.22948040e-06,\n",
       "         1.01955720e-07,  1.74839192e-06,  2.82863380e-06, -6.34109438e-06,\n",
       "         2.01277658e-06,  2.23341658e-06, -3.02401941e-06, -2.02362185e-06,\n",
       "         4.53830125e-07,  1.51460836e-06,  3.59589581e-06,  4.34188769e-06,\n",
       "         3.05909202e-06, -2.40545342e-06,  5.97225562e-06,  4.42126930e-06,\n",
       "        -7.07487561e-06,  6.40643748e-07,  5.64523407e-06, -3.59924479e-06,\n",
       "         3.60539389e-06, -6.31467628e-07,  4.64620598e-07, -3.75089883e-07,\n",
       "         2.92799632e-06, -1.62441688e-06, -1.25621500e-06, -5.77421133e-08,\n",
       "        -3.03361318e-08,  2.37243808e-06, -1.41252815e-06, -2.09705817e-08,\n",
       "        -5.80761434e-06, -3.86878082e-06, -2.73124851e-06, -2.73370347e-06,\n",
       "         1.50288679e-06, -7.81734570e-07,  1.84337352e-06, -5.35200252e-06,\n",
       "        -1.95415055e-06,  3.42254430e-06,  3.89352044e-06, -3.96805319e-07,\n",
       "        -4.07474573e-07, -7.67123856e-06,  3.42567569e-06, -1.30713875e-06,\n",
       "        -2.86026989e-06,  1.73171054e-06, -7.07603067e-06, -2.56565477e-06,\n",
       "        -8.58064254e-07, -2.81590110e-06, -2.21200295e-07,  2.87928901e-06,\n",
       "         1.23655184e-06, -1.52439270e-06, -1.70895544e-06,  7.83990401e-07,\n",
       "         4.02879493e-07, -3.75795207e-06,  7.04005970e-06, -7.64456945e-06,\n",
       "         2.28850831e-06, -2.28756835e-06,  7.37130108e-07,  2.41164207e-06,\n",
       "         4.11190831e-06, -8.20866603e-07,  5.89681940e-06,  2.08286997e-06,\n",
       "         3.33058097e-06,  1.12996372e-06,  3.03518186e-06,  2.84774774e-06,\n",
       "        -5.46805586e-06, -6.54398036e-07,  3.53585392e-06, -5.19607624e-07,\n",
       "         4.31873559e-07, -5.03820138e-06, -3.61886305e-06, -9.87114731e-07,\n",
       "         4.58675800e-07, -1.56649577e-07, -2.47696676e-06, -2.67718292e-06,\n",
       "         1.28444185e-06,  2.12338864e-06, -3.86699776e-06,  1.13507633e-06,\n",
       "        -7.44509975e-07,  4.25637381e-06,  6.07320135e-06,  2.37409654e-06,\n",
       "         3.64165885e-06,  6.29124997e-07, -3.88410899e-06,  6.19108187e-06,\n",
       "        -8.32642229e-07,  1.13475767e-06, -5.30003899e-06,  1.09995415e-06,\n",
       "        -1.96393830e-06,  1.46410821e-06, -7.95954111e-06, -3.62071489e-07,\n",
       "        -1.49000891e-06,  9.18001751e-06, -2.18604760e-06,  1.28186002e-07,\n",
       "         2.13708290e-06,  1.15036346e-06,  7.95906885e-07, -2.22155427e-06,\n",
       "        -1.17380750e-06, -4.42439296e-06,  5.72819658e-07,  1.81569362e-07,\n",
       "         6.90139643e-07,  9.98066753e-07, -1.64953190e-06,  4.50906327e-06,\n",
       "         5.92639208e-06, -4.13120688e-06,  4.46714284e-06,  4.13070939e-06,\n",
       "         1.21081939e-06,  1.87970522e-06,  5.28372721e-06, -6.48611149e-06,\n",
       "         3.61042476e-06,  2.07981589e-06, -4.33541800e-06, -2.36245342e-06,\n",
       "         1.05960191e-06,  1.37295530e-07,  3.25212227e-06,  4.97803649e-06,\n",
       "         2.51779920e-06, -5.43013982e-07,  7.29287967e-06,  7.20359139e-06,\n",
       "        -6.63557284e-06,  2.44704347e-06,  5.10420341e-06, -1.87587591e-06,\n",
       "         3.54276011e-07,  5.07611730e-07, -4.88728631e-08, -2.31895638e-06,\n",
       "         3.52748316e-07, -3.58934756e-07, -1.07154085e-06,  5.32820820e-07,\n",
       "        -1.99280947e-07, -9.73330884e-07, -1.68889176e-06, -2.61541095e-06,\n",
       "        -5.02517105e-06, -4.24389964e-06, -2.18345531e-06, -4.83949771e-06,\n",
       "         2.89467789e-06,  7.61771503e-07,  2.07376002e-06, -7.54425082e-06,\n",
       "        -1.92226435e-06, -9.73807232e-07,  5.90117361e-06,  4.15162248e-07,\n",
       "        -1.40785551e-06, -7.41243684e-06,  2.88382626e-06,  5.85737325e-09,\n",
       "        -4.64982213e-06,  2.14117767e-06, -6.62613957e-06, -4.15446402e-06,\n",
       "        -1.83061638e-06, -1.05787728e-06,  9.71505415e-07,  6.27687314e-06,\n",
       "        -1.32209914e-06, -5.79867265e-06, -2.62285153e-06,  7.45753198e-07],\n",
       "       dtype=float32),\n",
       " array([[-2.20294271e-04, -5.00143680e-04, -3.94527131e-04, ...,\n",
       "          4.97733311e-07,  5.02664875e-07,  4.93664629e-07],\n",
       "        [-3.77083597e-05, -1.07460124e-04,  4.99607177e-06, ...,\n",
       "         -7.12348083e-08, -7.19558173e-08, -7.06452994e-08],\n",
       "        [ 1.92147112e-04, -1.06386302e-04, -3.76198404e-05, ...,\n",
       "         -1.44510004e-08, -1.45966963e-08, -1.43450833e-08],\n",
       "        ...,\n",
       "        [-8.61231529e-05, -3.25123023e-04, -1.51674627e-04, ...,\n",
       "          1.65452491e-07,  1.67086654e-07,  1.64116528e-07],\n",
       "        [ 2.43809700e-05, -2.26091332e-04, -1.34555070e-04, ...,\n",
       "          7.92060533e-08,  7.99908619e-08,  7.85385552e-08],\n",
       "        [ 1.33865411e-04,  2.98660831e-04,  1.67619524e-04, ...,\n",
       "         -1.34321027e-07, -1.35666667e-07, -1.33229435e-07]], dtype=float32),\n",
       " array([-0.7813354 , -1.097997  , -0.98131204, ...,  0.00198236,\n",
       "         0.00200203,  0.00196618], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, action_type):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l1, forget_bias=0.0)\n",
    "        lstm_cell_l2 = tf.contrib.rnn.BasicLSTMCell(self.hidden_size_l2, forget_bias=0.0)\n",
    "        \n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "        # This is an optimization of the LSTM output, but is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            lstm_cell_l1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l1, output_keep_prob=keep_prob)\n",
    "            lstm_cell_l2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_l2, output_keep_prob=keep_prob)\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, self.embeding_vector_size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if action_type == \"is_training\" and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instantiating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, self.hidden_size_l2])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.hidden_size_l2, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        out_words = tf.argmax(prob, axis=2)\n",
    "        self._output_words = out_words\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "            \n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    \n",
    "#         loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "#                                                       [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if action_type != \"is_training\":\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "\n",
    "    \n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    # Returns the final output words for this model\n",
    "    @property\n",
    "    def final_output_words(self):\n",
    "        return self._output_words\n",
    "    \n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_one_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_one_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, out_words, _ = session.run([m.cost, m.final_state, m.final_output_words, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 4502.370 speed: 708 wps\n",
      "Itr 87 of 774, perplexity: 1284.250 speed: 760 wps\n",
      "Itr 164 of 774, perplexity: 983.511 speed: 758 wps\n",
      "Itr 241 of 774, perplexity: 822.482 speed: 757 wps\n",
      "Itr 318 of 774, perplexity: 727.542 speed: 760 wps\n",
      "Itr 395 of 774, perplexity: 650.601 speed: 764 wps\n",
      "Itr 472 of 774, perplexity: 589.091 speed: 769 wps\n",
      "Itr 549 of 774, perplexity: 535.054 speed: 768 wps\n",
      "Itr 626 of 774, perplexity: 491.741 speed: 769 wps\n",
      "Itr 703 of 774, perplexity: 457.371 speed: 770 wps\n",
      "Epoch 1 : Train Perplexity: 432.870\n",
      "Epoch 1 : Valid Perplexity: 241.805\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 269.780 speed: 728 wps\n",
      "Itr 87 of 774, perplexity: 238.806 speed: 774 wps\n",
      "Itr 164 of 774, perplexity: 228.444 speed: 781 wps\n",
      "Itr 241 of 774, perplexity: 219.701 speed: 781 wps\n",
      "Itr 318 of 774, perplexity: 217.055 speed: 781 wps\n",
      "Itr 395 of 774, perplexity: 211.350 speed: 777 wps\n",
      "Itr 472 of 774, perplexity: 207.098 speed: 778 wps\n",
      "Itr 549 of 774, perplexity: 200.387 speed: 779 wps\n",
      "Itr 626 of 774, perplexity: 194.874 speed: 781 wps\n",
      "Itr 703 of 774, perplexity: 190.797 speed: 785 wps\n",
      "Epoch 2 : Train Perplexity: 188.100\n",
      "Epoch 2 : Valid Perplexity: 174.677\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 186.718 speed: 768 wps\n",
      "Itr 87 of 774, perplexity: 161.221 speed: 810 wps\n",
      "Itr 164 of 774, perplexity: 157.592 speed: 797 wps\n",
      "Itr 241 of 774, perplexity: 152.963 speed: 797 wps\n",
      "Itr 318 of 774, perplexity: 153.015 speed: 795 wps\n",
      "Itr 395 of 774, perplexity: 150.305 speed: 794 wps\n",
      "Itr 472 of 774, perplexity: 148.807 speed: 794 wps\n",
      "Itr 549 of 774, perplexity: 144.961 speed: 793 wps\n",
      "Itr 626 of 774, perplexity: 142.144 speed: 794 wps\n",
      "Itr 703 of 774, perplexity: 140.294 speed: 793 wps\n",
      "Epoch 3 : Train Perplexity: 139.271\n",
      "Epoch 3 : Valid Perplexity: 150.286\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 148.288 speed: 811 wps\n",
      "Itr 87 of 774, perplexity: 128.523 speed: 810 wps\n",
      "Itr 164 of 774, perplexity: 126.477 speed: 801 wps\n",
      "Itr 241 of 774, perplexity: 123.283 speed: 803 wps\n",
      "Itr 318 of 774, perplexity: 123.842 speed: 814 wps\n",
      "Itr 395 of 774, perplexity: 121.895 speed: 811 wps\n",
      "Itr 472 of 774, perplexity: 121.089 speed: 807 wps\n",
      "Itr 549 of 774, perplexity: 118.295 speed: 806 wps\n",
      "Itr 626 of 774, perplexity: 116.338 speed: 804 wps\n",
      "Itr 703 of 774, perplexity: 115.185 speed: 801 wps\n",
      "Epoch 4 : Train Perplexity: 114.662\n",
      "Epoch 4 : Valid Perplexity: 140.432\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 774, perplexity: 128.043 speed: 773 wps\n"
     ]
    }
   ],
   "source": [
    "# Initializes the Execution Graph and the Session\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(\"is_training\")\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(\"is_validating\")\n",
    "        mtest = PTBModel(\"is_testing\")\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        # Define the decay for this epoch\n",
    "        lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for this epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "        \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_one_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_one_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_one_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "    print('Training time in minutes:' (end - start)/60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
